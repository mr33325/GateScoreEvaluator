{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8def335d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Marks Obtained: 44.333333333333336/100\n",
      "[1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 16, 19, 20, 21, 22, 24, 27, 28, 31, 32, 35, 36, 40, 41, 44, 46, 47, 58, 59, 61, 62, 63]\n",
      "[5, 7, 14, 17, 23, 26, 33, 37, 39, 42, 53, 54, 57]\n",
      "[15, 18, 25, 29, 30, 34, 38, 43, 45, 48, 49, 50, 51, 52, 55, 56, 60, 64, 65]\n",
      "HTML page created: exam_summary.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import webbrowser\n",
    "\n",
    "# Load the answer sheet and answer key\n",
    "answer_sheet_df = pd.read_excel('answer_sheet.xlsx', usecols=['Q. No.', 'Answer'])\n",
    "answer_key_df = pd.read_excel('answer_key.xlsx', usecols=['Q. No.', 'Question Type', 'Key', 'Mark'])\n",
    "\n",
    "# Merge the answer sheet and answer key on question number\n",
    "merged_df = pd.merge(answer_sheet_df, answer_key_df, on='Q. No.')\n",
    "\n",
    "correct= []\n",
    "wrong= []\n",
    "not_attempted= []\n",
    "\n",
    "# Function to calculate marks based on the given rules\n",
    "def calculate_marks(row):\n",
    "    if row['Question Type'] == 'MCQ':\n",
    "        if row['Key'] == 'MTA':\n",
    "            correct.append(row['Q. No.'])\n",
    "            return row['Mark']\n",
    "        elif pd.isnull(row['Answer']):\n",
    "            not_attempted.append(row['Q. No.'])\n",
    "            return 0\n",
    "        elif row['Answer'] == row['Key']:\n",
    "            correct.append(row['Q. No.'])\n",
    "            return row['Mark']\n",
    "        else:\n",
    "            wrong.append(row['Q. No.'])\n",
    "            return -1 / 3 if row['Mark'] == 1 else -2 / 3\n",
    "    elif row['Question Type'] == 'MSQ':\n",
    "        if pd.isnull(row['Answer']):\n",
    "            not_attempted.append(row['Q. No.'])\n",
    "            return 0\n",
    "        \n",
    "        given_answer_list = row['Answer'].split(', ')  # Assuming answers are separated by ', ' in the answer sheet\n",
    "        key_list = row['Key'].split(', ')  # Assuming keys are separated by ', ' in the answer key\n",
    "        \n",
    "        if len(given_answer_list)==len(key_list) and all(answer in key_list for answer in given_answer_list):\n",
    "            correct.append(row['Q. No.'])\n",
    "            return row['Mark']\n",
    "        else:\n",
    "            wrong.append(row['Q. No.'])\n",
    "            return 0\n",
    "    elif row['Question Type'] == 'NAT':\n",
    "        if pd.isnull(row['Answer']):\n",
    "            not_attempted.append(row['Q. No.'])\n",
    "            return 0\n",
    "        \n",
    "        if \"OR\" in row['Key']:\n",
    "            range_ans = list(set(int(value) for value in re.split(r'\\s+to\\s+|\\s+OR\\s+', row['Key'])))\n",
    "            answer = int(row['Answer'])\n",
    "            \n",
    "            if answer in range_ans:\n",
    "                correct.append(row['Q. No.'])\n",
    "                return row['Mark']\n",
    "            else:\n",
    "                wrong.append(row['Q. No.'])\n",
    "                return 0\n",
    "\n",
    "        else:\n",
    "            range_start, range_end = map(int, row['Key'].split(' to '))\n",
    "            answer = int(row['Answer'])\n",
    "            \n",
    "            if range_start <= answer <= range_end:\n",
    "                correct.append(row['Q. No.'])\n",
    "                return row['Mark']\n",
    "            else:\n",
    "                wrong.append(row['Q. No.'])\n",
    "                return 0\n",
    "\n",
    "# Apply the calculate_marks function to each row and calculate total marks\n",
    "merged_df['Obtained Marks'] = merged_df.apply(calculate_marks, axis=1)\n",
    "\n",
    "# Calculate total marks\n",
    "total_marks = merged_df['Obtained Marks'].sum()\n",
    "\n",
    "# Print the total marks\n",
    "print('Total Marks Obtained: '+ str(total_marks) + '/100')\n",
    "print(correct)\n",
    "print(wrong)\n",
    "print(not_attempted)\n",
    "merged_df = merged_df[['Q. No.', 'Answer', 'Key', 'Question Type', 'Mark', 'Obtained Marks']]\n",
    "merged_df.to_excel(\"MarksData.xlsx\")\n",
    "merged_df.to_json('MarksData.json')\n",
    "###--------------------------------------- HTML---------------------------------------------------------- ###\n",
    "correct_questions= correct\n",
    "wrong_questions= wrong\n",
    "not_attempted_questions= not_attempted\n",
    "# Calculate additional statistics\n",
    "total_questions = len(correct_questions) + len(wrong_questions) + len(not_attempted_questions)\n",
    "questions_attempted = len(correct_questions) + len(wrong_questions)  # Corrected this line\n",
    "percentage_correct = (len(correct_questions) / questions_attempted) * 100 if questions_attempted > 0 else 0\n",
    "percentage_wrong = (len(wrong_questions) / questions_attempted) * 100 if questions_attempted > 0 else 0\n",
    "percentage_not_attempted = (len(not_attempted_questions) / total_questions) * 100\n",
    "\n",
    "# Create HTML content (transposed table with additional statistics)\n",
    "with open(\"index.html\", \"r\") as html_file:\n",
    "    html_content = html_file.read()\n",
    "\n",
    "# Format question lists for HTML display\n",
    "correct_str = \", \".join(map(str, correct_questions))\n",
    "wrong_str = \", \".join(map(str, wrong_questions))\n",
    "not_attempted_str = \", \".join(map(str, not_attempted_questions))\n",
    "\n",
    "# Populate HTML content with formatted question numbers and statistics\n",
    "html_content = html_content.format(total_marks, correct_str, wrong_str, not_attempted_str,\n",
    "                                   total_questions, questions_attempted,\n",
    "                                   len(correct_questions), percentage_correct, \n",
    "                                   len(wrong_questions), percentage_wrong,\n",
    "                                   len(not_attempted_questions), percentage_not_attempted,x)\n",
    "\n",
    "# Write the HTML content to a file\n",
    "with open(\"exam_summary.html\", \"w\") as html_file:\n",
    "    html_file.write(html_content)\n",
    "\n",
    "print(\"HTML page created: exam_summary.html\")\n",
    "webbrowser.open('exam_summary.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ff9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb882d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
